{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab9087d",
   "metadata": {},
   "source": [
    "Predicting IDRL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "3aeed760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1_z025</th>\n",
       "      <th>I1_z050</th>\n",
       "      <th>I1_y125</th>\n",
       "      <th>I1_y150</th>\n",
       "      <th>I1_y175</th>\n",
       "      <th>I1_z075</th>\n",
       "      <th>I2_z075</th>\n",
       "      <th>I3_z075</th>\n",
       "      <th>I4_z075</th>\n",
       "      <th>I5_z075</th>\n",
       "      <th>...</th>\n",
       "      <th>I95_z075</th>\n",
       "      <th>Est.STLE</th>\n",
       "      <th>p_mean</th>\n",
       "      <th>p_med</th>\n",
       "      <th>p_std</th>\n",
       "      <th>p_var</th>\n",
       "      <th>p_q1</th>\n",
       "      <th>p_q3</th>\n",
       "      <th>p_max</th>\n",
       "      <th>p_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>6.954271</td>\n",
       "      <td>7.091257</td>\n",
       "      <td>7.247820</td>\n",
       "      <td>7.113580</td>\n",
       "      <td>7.110719</td>\n",
       "      <td>6.890042</td>\n",
       "      <td>6.912281</td>\n",
       "      <td>7.208522</td>\n",
       "      <td>7.096931</td>\n",
       "      <td>7.035995</td>\n",
       "      <td>...</td>\n",
       "      <td>21.945808</td>\n",
       "      <td>24.352679</td>\n",
       "      <td>8.963511</td>\n",
       "      <td>8.469819</td>\n",
       "      <td>2.627361</td>\n",
       "      <td>6.903025</td>\n",
       "      <td>7.584825</td>\n",
       "      <td>9.264459</td>\n",
       "      <td>21.945808</td>\n",
       "      <td>6.890042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>6.923579</td>\n",
       "      <td>7.087839</td>\n",
       "      <td>7.257635</td>\n",
       "      <td>7.087724</td>\n",
       "      <td>7.117243</td>\n",
       "      <td>6.886631</td>\n",
       "      <td>6.895090</td>\n",
       "      <td>7.194828</td>\n",
       "      <td>7.124069</td>\n",
       "      <td>7.012246</td>\n",
       "      <td>...</td>\n",
       "      <td>25.566223</td>\n",
       "      <td>23.353871</td>\n",
       "      <td>9.516271</td>\n",
       "      <td>8.465357</td>\n",
       "      <td>3.930882</td>\n",
       "      <td>15.451832</td>\n",
       "      <td>7.577978</td>\n",
       "      <td>9.300327</td>\n",
       "      <td>25.566223</td>\n",
       "      <td>6.886631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>6.886065</td>\n",
       "      <td>7.043405</td>\n",
       "      <td>7.195471</td>\n",
       "      <td>7.090956</td>\n",
       "      <td>7.081362</td>\n",
       "      <td>6.876395</td>\n",
       "      <td>6.867582</td>\n",
       "      <td>7.191404</td>\n",
       "      <td>7.124069</td>\n",
       "      <td>7.005461</td>\n",
       "      <td>...</td>\n",
       "      <td>29.069303</td>\n",
       "      <td>22.105950</td>\n",
       "      <td>10.496363</td>\n",
       "      <td>8.486814</td>\n",
       "      <td>5.704561</td>\n",
       "      <td>32.542020</td>\n",
       "      <td>7.564351</td>\n",
       "      <td>9.345553</td>\n",
       "      <td>29.069303</td>\n",
       "      <td>6.867582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>6.848552</td>\n",
       "      <td>7.046823</td>\n",
       "      <td>7.234733</td>\n",
       "      <td>7.071564</td>\n",
       "      <td>7.078100</td>\n",
       "      <td>6.913925</td>\n",
       "      <td>6.915720</td>\n",
       "      <td>7.239335</td>\n",
       "      <td>7.103715</td>\n",
       "      <td>6.988497</td>\n",
       "      <td>...</td>\n",
       "      <td>31.451757</td>\n",
       "      <td>20.627715</td>\n",
       "      <td>11.737188</td>\n",
       "      <td>8.593003</td>\n",
       "      <td>7.353198</td>\n",
       "      <td>54.069515</td>\n",
       "      <td>7.605122</td>\n",
       "      <td>9.751926</td>\n",
       "      <td>31.451757</td>\n",
       "      <td>6.848552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>6.906527</td>\n",
       "      <td>7.050240</td>\n",
       "      <td>7.244548</td>\n",
       "      <td>7.065100</td>\n",
       "      <td>7.100933</td>\n",
       "      <td>6.879807</td>\n",
       "      <td>6.884775</td>\n",
       "      <td>7.191404</td>\n",
       "      <td>7.134245</td>\n",
       "      <td>7.002068</td>\n",
       "      <td>...</td>\n",
       "      <td>31.907745</td>\n",
       "      <td>20.115385</td>\n",
       "      <td>12.095411</td>\n",
       "      <td>8.602455</td>\n",
       "      <td>7.736227</td>\n",
       "      <td>59.849206</td>\n",
       "      <td>7.590816</td>\n",
       "      <td>9.921560</td>\n",
       "      <td>31.907745</td>\n",
       "      <td>6.879807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>11.114364</td>\n",
       "      <td>11.393116</td>\n",
       "      <td>11.592338</td>\n",
       "      <td>11.438862</td>\n",
       "      <td>11.464869</td>\n",
       "      <td>11.032471</td>\n",
       "      <td>11.039554</td>\n",
       "      <td>11.624320</td>\n",
       "      <td>11.420302</td>\n",
       "      <td>11.286018</td>\n",
       "      <td>...</td>\n",
       "      <td>41.138736</td>\n",
       "      <td>22.402067</td>\n",
       "      <td>15.146593</td>\n",
       "      <td>13.462253</td>\n",
       "      <td>6.252376</td>\n",
       "      <td>39.092202</td>\n",
       "      <td>12.093513</td>\n",
       "      <td>14.624408</td>\n",
       "      <td>41.138736</td>\n",
       "      <td>11.032471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>11.121184</td>\n",
       "      <td>11.393116</td>\n",
       "      <td>11.664303</td>\n",
       "      <td>11.429163</td>\n",
       "      <td>11.393123</td>\n",
       "      <td>11.073422</td>\n",
       "      <td>11.022379</td>\n",
       "      <td>11.579826</td>\n",
       "      <td>11.420302</td>\n",
       "      <td>11.299583</td>\n",
       "      <td>...</td>\n",
       "      <td>39.420860</td>\n",
       "      <td>22.451419</td>\n",
       "      <td>14.937114</td>\n",
       "      <td>13.424858</td>\n",
       "      <td>5.655499</td>\n",
       "      <td>31.984669</td>\n",
       "      <td>12.144211</td>\n",
       "      <td>14.658052</td>\n",
       "      <td>39.420860</td>\n",
       "      <td>11.022379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>11.857625</td>\n",
       "      <td>11.889174</td>\n",
       "      <td>12.367591</td>\n",
       "      <td>12.376487</td>\n",
       "      <td>12.439930</td>\n",
       "      <td>12.042598</td>\n",
       "      <td>12.320576</td>\n",
       "      <td>12.589452</td>\n",
       "      <td>12.843768</td>\n",
       "      <td>13.062776</td>\n",
       "      <td>...</td>\n",
       "      <td>14.547205</td>\n",
       "      <td>5.474044</td>\n",
       "      <td>14.456023</td>\n",
       "      <td>14.713852</td>\n",
       "      <td>0.756642</td>\n",
       "      <td>0.572508</td>\n",
       "      <td>14.557581</td>\n",
       "      <td>14.847067</td>\n",
       "      <td>15.001857</td>\n",
       "      <td>11.857625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>12.522442</td>\n",
       "      <td>12.501598</td>\n",
       "      <td>12.619462</td>\n",
       "      <td>12.631915</td>\n",
       "      <td>12.746461</td>\n",
       "      <td>12.561321</td>\n",
       "      <td>12.918044</td>\n",
       "      <td>13.212308</td>\n",
       "      <td>13.460502</td>\n",
       "      <td>13.649306</td>\n",
       "      <td>...</td>\n",
       "      <td>14.528219</td>\n",
       "      <td>5.474044</td>\n",
       "      <td>14.494660</td>\n",
       "      <td>14.684112</td>\n",
       "      <td>0.584053</td>\n",
       "      <td>0.341118</td>\n",
       "      <td>14.573390</td>\n",
       "      <td>14.789135</td>\n",
       "      <td>14.958830</td>\n",
       "      <td>12.501598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>13.238369</td>\n",
       "      <td>13.213307</td>\n",
       "      <td>13.430659</td>\n",
       "      <td>13.449940</td>\n",
       "      <td>13.542110</td>\n",
       "      <td>13.274571</td>\n",
       "      <td>13.419313</td>\n",
       "      <td>13.581905</td>\n",
       "      <td>13.758683</td>\n",
       "      <td>13.923911</td>\n",
       "      <td>...</td>\n",
       "      <td>14.551003</td>\n",
       "      <td>3.713793</td>\n",
       "      <td>14.524730</td>\n",
       "      <td>14.642594</td>\n",
       "      <td>0.373980</td>\n",
       "      <td>0.139861</td>\n",
       "      <td>14.567539</td>\n",
       "      <td>14.709727</td>\n",
       "      <td>14.815405</td>\n",
       "      <td>13.213307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1476 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        I1_z025    I1_z050    I1_y125    I1_y150    I1_y175    I1_z075  \\\n",
       "2152   6.954271   7.091257   7.247820   7.113580   7.110719   6.890042   \n",
       "2153   6.923579   7.087839   7.257635   7.087724   7.117243   6.886631   \n",
       "2154   6.886065   7.043405   7.195471   7.090956   7.081362   6.876395   \n",
       "2155   6.848552   7.046823   7.234733   7.071564   7.078100   6.913925   \n",
       "2156   6.906527   7.050240   7.244548   7.065100   7.100933   6.879807   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "4195  11.114364  11.393116  11.592338  11.438862  11.464869  11.032471   \n",
       "4196  11.121184  11.393116  11.664303  11.429163  11.393123  11.073422   \n",
       "4375  11.857625  11.889174  12.367591  12.376487  12.439930  12.042598   \n",
       "4376  12.522442  12.501598  12.619462  12.631915  12.746461  12.561321   \n",
       "4377  13.238369  13.213307  13.430659  13.449940  13.542110  13.274571   \n",
       "\n",
       "        I2_z075    I3_z075    I4_z075    I5_z075  ...   I95_z075   Est.STLE  \\\n",
       "2152   6.912281   7.208522   7.096931   7.035995  ...  21.945808  24.352679   \n",
       "2153   6.895090   7.194828   7.124069   7.012246  ...  25.566223  23.353871   \n",
       "2154   6.867582   7.191404   7.124069   7.005461  ...  29.069303  22.105950   \n",
       "2155   6.915720   7.239335   7.103715   6.988497  ...  31.451757  20.627715   \n",
       "2156   6.884775   7.191404   7.134245   7.002068  ...  31.907745  20.115385   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "4195  11.039554  11.624320  11.420302  11.286018  ...  41.138736  22.402067   \n",
       "4196  11.022379  11.579826  11.420302  11.299583  ...  39.420860  22.451419   \n",
       "4375  12.320576  12.589452  12.843768  13.062776  ...  14.547205   5.474044   \n",
       "4376  12.918044  13.212308  13.460502  13.649306  ...  14.528219   5.474044   \n",
       "4377  13.419313  13.581905  13.758683  13.923911  ...  14.551003   3.713793   \n",
       "\n",
       "         p_mean      p_med     p_std      p_var       p_q1       p_q3  \\\n",
       "2152   8.963511   8.469819  2.627361   6.903025   7.584825   9.264459   \n",
       "2153   9.516271   8.465357  3.930882  15.451832   7.577978   9.300327   \n",
       "2154  10.496363   8.486814  5.704561  32.542020   7.564351   9.345553   \n",
       "2155  11.737188   8.593003  7.353198  54.069515   7.605122   9.751926   \n",
       "2156  12.095411   8.602455  7.736227  59.849206   7.590816   9.921560   \n",
       "...         ...        ...       ...        ...        ...        ...   \n",
       "4195  15.146593  13.462253  6.252376  39.092202  12.093513  14.624408   \n",
       "4196  14.937114  13.424858  5.655499  31.984669  12.144211  14.658052   \n",
       "4375  14.456023  14.713852  0.756642   0.572508  14.557581  14.847067   \n",
       "4376  14.494660  14.684112  0.584053   0.341118  14.573390  14.789135   \n",
       "4377  14.524730  14.642594  0.373980   0.139861  14.567539  14.709727   \n",
       "\n",
       "          p_max      p_min  \n",
       "2152  21.945808   6.890042  \n",
       "2153  25.566223   6.886631  \n",
       "2154  29.069303   6.867582  \n",
       "2155  31.451757   6.848552  \n",
       "2156  31.907745   6.879807  \n",
       "...         ...        ...  \n",
       "4195  41.138736  11.032471  \n",
       "4196  39.420860  11.022379  \n",
       "4375  15.001857  11.857625  \n",
       "4376  14.958830  12.501598  \n",
       "4377  14.815405  13.213307  \n",
       "\n",
       "[1476 rows x 109 columns]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "aadcb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "559b72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "idrl = pd.read_csv('../data/idrl_estimated', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "210f27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idrl = idrl.loc[idrl.notna()['Est.STLE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "675c5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idrl.drop('Est.STLE',axis=1)\n",
    "y = idrl['Est.STLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "27243bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "02b10d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaeb4c8",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "13e03f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "5d88d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "c8d60e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-42 {color: black;background-color: white;}#sk-container-id-42 pre{padding: 0;}#sk-container-id-42 div.sk-toggleable {background-color: white;}#sk-container-id-42 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-42 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-42 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-42 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-42 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-42 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-42 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-42 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-42 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-42 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-42 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-42 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-42 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-42 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-42 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-42 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-42 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-42 div.sk-item {position: relative;z-index: 1;}#sk-container-id-42 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-42 div.sk-item::before, #sk-container-id-42 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-42 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-42 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-42 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-42 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-42 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-42 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-42 div.sk-label-container {text-align: center;}#sk-container-id-42 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-42 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-42\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" checked><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "22b655cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5b3a10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:0.8952783700799554\n",
      "MSE:4.2625215184354355\n"
     ]
    }
   ],
   "source": [
    "print('R2:'+str(lin_reg.score(X_test,y_test)))\n",
    "print('MSE:'+str(mean_squared_error(y_test, lin_reg.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f5583",
   "metadata": {},
   "source": [
    "Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7e31218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "96079e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "358058ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "70d3307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "las_reg = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f68234c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-25 {color: black;background-color: white;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Lasso()"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1b784d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:0.8657108274632882\n",
      "MSE:5.466019656757259\n"
     ]
    }
   ],
   "source": [
    "print('R2:'+str(las_reg.score(X_test_scaled,y_test)))\n",
    "print('MSE:'+str(mean_squared_error(y_test, las_reg.predict(X_test_scaled))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "56bdb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "rid_reg = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "04997585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-26 {color: black;background-color: white;}#sk-container-id-26 pre{padding: 0;}#sk-container-id-26 div.sk-toggleable {background-color: white;}#sk-container-id-26 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-26 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-26 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-26 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-26 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-26 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-26 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-26 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-26 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-26 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-26 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-26 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-26 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-26 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-26 div.sk-item {position: relative;z-index: 1;}#sk-container-id-26 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-26 div.sk-item::before, #sk-container-id-26 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-26 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-26 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-26 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-26 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-26 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-26 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-26 div.sk-label-container {text-align: center;}#sk-container-id-26 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-26 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-26\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" checked><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rid_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1f22a369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:0.9467426595074573\n",
      "MSE:2.167752354861451\n"
     ]
    }
   ],
   "source": [
    "print('R2:'+str(rid_reg.score(X_test_scaled,y_test)))\n",
    "print('MSE:'+str(mean_squared_error(y_test, rid_reg.predict(X_test_scaled))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61469647",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f6dde9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b64ba29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "84be1c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-29 {color: black;background-color: white;}#sk-container-id-29 pre{padding: 0;}#sk-container-id-29 div.sk-toggleable {background-color: white;}#sk-container-id-29 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-29 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-29 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-29 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-29 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-29 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-29 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-29 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-29 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-29 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-29 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-29 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-29 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-29 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-29 div.sk-item {position: relative;z-index: 1;}#sk-container-id-29 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-29 div.sk-item::before, #sk-container-id-29 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-29 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-29 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-29 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-29 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-29 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-29 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-29 div.sk-label-container {text-align: center;}#sk-container-id-29 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-29 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-29\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" checked><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d0a220d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:0.9819490775678188\n",
      "MSE:0.734733076189965\n"
     ]
    }
   ],
   "source": [
    "print('R2:'+str(rfr.score(X_test,y_test)))\n",
    "print('MSE:'+str(mean_squared_error(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "faf5fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestRegressor(random_state=146)\n",
    "param_grid = dict(n_estimators=[1,100,500],max_depth = [2,3,4,5],\n",
    "                 min_samples_split = [2,3,4,5])\n",
    "cv = KFold(n_splits=10,random_state=146,shuffle=True)\n",
    "grid = GridSearchCV(rfc,param_grid,cv=cv,scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "25312f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-30 {color: black;background-color: white;}#sk-container-id-30 pre{padding: 0;}#sk-container-id-30 div.sk-toggleable {background-color: white;}#sk-container-id-30 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-30 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-30 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-30 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-30 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-30 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-30 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-30 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-30 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-30 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-30 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-30 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-30 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-30 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-30 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-30 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-30 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-30 div.sk-item {position: relative;z-index: 1;}#sk-container-id-30 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-30 div.sk-item::before, #sk-container-id-30 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-30 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-30 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-30 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-30 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-30 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-30 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-30 div.sk-label-container {text-align: center;}#sk-container-id-30 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-30 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-30\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=KFold(n_splits=10, random_state=146, shuffle=True),\n",
       "             estimator=RandomForestRegressor(random_state=146),\n",
       "             param_grid={&#x27;max_depth&#x27;: [2, 3, 4, 5],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [1, 100, 500]},\n",
       "             scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=KFold(n_splits=10, random_state=146, shuffle=True),\n",
       "             estimator=RandomForestRegressor(random_state=146),\n",
       "             param_grid={&#x27;max_depth&#x27;: [2, 3, 4, 5],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [1, 100, 500]},\n",
       "             scoring=&#x27;r2&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=146)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=146)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=146, shuffle=True),\n",
       "             estimator=RandomForestRegressor(random_state=146),\n",
       "             param_grid={'max_depth': [2, 3, 4, 5],\n",
       "                         'min_samples_split': [2, 3, 4, 5],\n",
       "                         'n_estimators': [1, 100, 500]},\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "eb6e7844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 1} with a score of 1.00\n"
     ]
    }
   ],
   "source": [
    "print(f'The best parameters are {grid.best_params_} with a score of {grid.best_score_:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e130e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)[['param_n_estimators','param_max_depth',\n",
    "                                'param_min_samples_split','mean_test_score','rank_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0f1bda4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.997771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.997771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_n_estimators param_max_depth param_min_samples_split  \\\n",
       "36                  1               5                       2   \n",
       "39                  1               5                       3   \n",
       "42                  1               5                       4   \n",
       "45                  1               5                       5   \n",
       "\n",
       "    mean_test_score  rank_test_score  \n",
       "36         0.997771                1  \n",
       "39         0.997771                1  \n",
       "42         0.997771                1  \n",
       "45         0.997771                1  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['rank_test_score'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e8fc1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_pressures = list(idrl.corr()['Est.STLE'].sort_values().head().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c60ec",
   "metadata": {},
   "source": [
    "Random Forest with only 5 Highly Correlated Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1171f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=1, max_depth=5, min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "9e45e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idrl[five_pressures]\n",
    "y = idrl['Est.STLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ebd7c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "45d817c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-36 {color: black;background-color: white;}#sk-container-id-36 pre{padding: 0;}#sk-container-id-36 div.sk-toggleable {background-color: white;}#sk-container-id-36 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-36 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-36 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-36 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-36 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-36 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-36 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-36 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-36 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-36 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-36 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-36 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-36 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-36 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-36 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-36 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-36 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-36 div.sk-item {position: relative;z-index: 1;}#sk-container-id-36 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-36 div.sk-item::before, #sk-container-id-36 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-36 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-36 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-36 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-36 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-36 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-36 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-36 div.sk-label-container {text-align: center;}#sk-container-id-36 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-36 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-36\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=5, n_estimators=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" checked><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=5, n_estimators=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=5, n_estimators=1)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "3c8559b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9676367161215476"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3bd64ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3172941831068041"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, rfr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cfed3",
   "metadata": {},
   "source": [
    "New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "76618894",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_col = idrl.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "324885da",
   "metadata": {},
   "outputs": [],
   "source": [
    "idrl['p_mean'] = idrl[og_col].mean(axis=1)\n",
    "idrl['p_med'] = idrl[og_col].median(axis=1)\n",
    "idrl['p_std'] = idrl[og_col].std(axis=1)\n",
    "idrl['p_var'] = idrl[og_col].var(axis=1)\n",
    "idrl['p_q1'] = idrl[og_col].quantile(0.25,axis=1)\n",
    "idrl['p_q3'] = idrl[og_col].quantile(0.75,axis=1)\n",
    "idrl['p_max'] = idrl[og_col].max(axis=1)\n",
    "idrl['p_min'] = idrl[og_col].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "2ebfbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=1, max_depth=5, min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "bae5a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idrl[['p_mean','p_med','p_std','p_var','p_q1','p_q3','p_max','p_min']]\n",
    "y = idrl['Est.STLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "b79b7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "9199f25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-43 {color: black;background-color: white;}#sk-container-id-43 pre{padding: 0;}#sk-container-id-43 div.sk-toggleable {background-color: white;}#sk-container-id-43 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-43 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-43 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-43 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-43 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-43 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-43 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-43 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-43 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-43 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-43 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-43 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-43 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-43 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-43 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-43 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-43 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-43 div.sk-item {position: relative;z-index: 1;}#sk-container-id-43 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-43 div.sk-item::before, #sk-container-id-43 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-43 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-43 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-43 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-43 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-43 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-43 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-43 div.sk-label-container {text-align: center;}#sk-container-id-43 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-43 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-43\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=5, n_estimators=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" checked><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=5, n_estimators=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=5, n_estimators=1)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b5b5e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:0.9850476108938276\n",
      "MSE:0.608612378987425\n"
     ]
    }
   ],
   "source": [
    "print('R2:'+str(rfr.score(X_test,y_test)))\n",
    "print('MSE:'+str(mean_squared_error(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "51e599cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "847e39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idrl[['p_mean','p_med','p_std','p_var','p_q1','p_q3','p_max','p_min']]\n",
    "y = idrl['Est.STLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "0a1a2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f498a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 340.9520 - val_loss: 141.1881\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.1471 - val_loss: 118.4460\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.8901 - val_loss: 174.6385\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.7287 - val_loss: 117.3928\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 98.6365 - val_loss: 86.8955\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 92.4069 - val_loss: 84.3566\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 92.0518 - val_loss: 80.6602\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 85.6486 - val_loss: 77.8223\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 81.1774 - val_loss: 79.2648\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 79.1539 - val_loss: 76.3108\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 75.5641 - val_loss: 70.1920\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 71.8447 - val_loss: 66.4341\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 69.1807 - val_loss: 63.0019\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 65.6456 - val_loss: 60.1153\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 61.7002 - val_loss: 56.8437\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 57.4894 - val_loss: 52.4214\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 52.4084 - val_loss: 46.7869\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 47.4557 - val_loss: 42.5770\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 43.9108 - val_loss: 39.5071\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 40.7066 - val_loss: 36.9966\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 38.2807 - val_loss: 34.7418\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 36.1098 - val_loss: 32.3922\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 33.6384 - val_loss: 30.4036\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 30.9102 - val_loss: 28.0476\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 28.6348 - val_loss: 25.8553\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 26.1898 - val_loss: 23.7433\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 24.0539 - val_loss: 21.8176\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 21.8484 - val_loss: 20.0299\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 19.8779 - val_loss: 18.3246\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 17.9612 - val_loss: 16.7238\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 16.2911 - val_loss: 15.1887\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 14.6717 - val_loss: 13.8461\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 13.2596 - val_loss: 12.4399\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 11.8020 - val_loss: 11.2620\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 10.4171 - val_loss: 10.0114\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 9.4200 - val_loss: 9.0323\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 8.5127 - val_loss: 8.0507\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 7.7396 - val_loss: 7.6511\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 7.1838 - val_loss: 6.9812\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6.8668 - val_loss: 6.6725\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6.5289 - val_loss: 6.2684\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6.2092 - val_loss: 5.8815\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5.7141 - val_loss: 5.7839\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5.4060 - val_loss: 5.4320\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5.3826 - val_loss: 5.6193\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5.1750 - val_loss: 5.0197\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5.2446 - val_loss: 5.0374\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5.0033 - val_loss: 4.6948\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.6894 - val_loss: 4.5191\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.2839 - val_loss: 4.7805\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.2107 - val_loss: 4.2623\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.0818 - val_loss: 4.3434\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.8071 - val_loss: 3.9201\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.5981 - val_loss: 3.9278\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.4940 - val_loss: 3.7186\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.3221 - val_loss: 3.5567\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.1442 - val_loss: 3.4529\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.9917 - val_loss: 3.3033\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.8728 - val_loss: 3.2033\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.7684 - val_loss: 3.0659\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.6263 - val_loss: 3.0257\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.5586 - val_loss: 2.8949\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.5670 - val_loss: 2.7862\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.5261 - val_loss: 2.7440\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.3057 - val_loss: 2.6223\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1803 - val_loss: 2.5763\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.0132 - val_loss: 2.4967\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.9729 - val_loss: 2.4813\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.0077 - val_loss: 2.3839\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8693 - val_loss: 2.3218\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8852 - val_loss: 2.2741\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8160 - val_loss: 2.2412\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8022 - val_loss: 2.1851\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8134 - val_loss: 2.1602\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8172 - val_loss: 2.1923\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.7166 - val_loss: 2.2369\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.7813 - val_loss: 2.2844\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8371 - val_loss: 2.2410\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.6379 - val_loss: 2.0484\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.5440 - val_loss: 2.1209\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4285 - val_loss: 1.9485\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3228 - val_loss: 1.9259\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3151 - val_loss: 1.9044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2800 - val_loss: 1.8793\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.2482 - val_loss: 1.8607\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2299 - val_loss: 1.8462\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2111 - val_loss: 1.8208\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2161 - val_loss: 1.8025\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2014 - val_loss: 1.7873\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1957 - val_loss: 1.7888\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1558 - val_loss: 1.7635\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1694 - val_loss: 1.7845\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1615 - val_loss: 1.7674\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1138 - val_loss: 1.7930\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1325 - val_loss: 1.6939\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0629 - val_loss: 1.6738\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0602 - val_loss: 1.6653\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0629 - val_loss: 1.6670\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0362 - val_loss: 1.6332\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0103 - val_loss: 1.6424\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0270 - val_loss: 1.6320\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9996 - val_loss: 1.6159\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0173 - val_loss: 1.5967\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9998 - val_loss: 1.5779\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9498 - val_loss: 1.5603\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9391 - val_loss: 1.5515\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9375 - val_loss: 1.5701\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9518 - val_loss: 1.5802\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9275 - val_loss: 1.6123\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9339 - val_loss: 1.5698\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9358 - val_loss: 1.5889\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8812 - val_loss: 1.5136\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8825 - val_loss: 1.5081\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8855 - val_loss: 1.4769\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9046 - val_loss: 1.5207\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8578 - val_loss: 1.4862\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8601 - val_loss: 1.4645\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8671 - val_loss: 1.4873\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9051 - val_loss: 1.5539\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8329 - val_loss: 1.4956\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8540 - val_loss: 1.4466\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8259 - val_loss: 1.4537\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8068 - val_loss: 1.4378\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7931 - val_loss: 1.4023\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7669 - val_loss: 1.4029\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7747 - val_loss: 1.4105\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7632 - val_loss: 1.4068\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7707 - val_loss: 1.4461\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7635 - val_loss: 1.5213\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7972 - val_loss: 1.3737\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7347 - val_loss: 1.3603\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7340 - val_loss: 1.3995\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7277 - val_loss: 1.3782\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7159 - val_loss: 1.3604\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7381 - val_loss: 1.3873\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7493 - val_loss: 1.4333\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7563 - val_loss: 1.4426\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7170 - val_loss: 1.3440\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6885 - val_loss: 1.3469\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6883 - val_loss: 1.3973\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7275 - val_loss: 1.3313\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6806 - val_loss: 1.3493\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6731 - val_loss: 1.3507\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7295 - val_loss: 1.3067\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6797 - val_loss: 1.3081\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6550 - val_loss: 1.3566\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6584 - val_loss: 1.3022\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6240 - val_loss: 1.3025\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6558 - val_loss: 1.2982\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6301 - val_loss: 1.2905\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6152 - val_loss: 1.2763\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6335 - val_loss: 1.2833\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6367 - val_loss: 1.3538\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6443 - val_loss: 1.2706\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6037 - val_loss: 1.2737\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5992 - val_loss: 1.3018\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6577 - val_loss: 1.2726\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6236 - val_loss: 1.2905\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6319 - val_loss: 1.2760\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6245 - val_loss: 1.3164\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5797 - val_loss: 1.2563\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5974 - val_loss: 1.2697\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6025 - val_loss: 1.3671\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6156 - val_loss: 1.2435\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5793 - val_loss: 1.2659\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5681 - val_loss: 1.2875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5791 - val_loss: 1.2357\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5484 - val_loss: 1.2513\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5597 - val_loss: 1.2892\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6118 - val_loss: 1.2402\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5977 - val_loss: 1.3506\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6218 - val_loss: 1.4274\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5921 - val_loss: 1.2242\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6125 - val_loss: 1.4154\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7401 - val_loss: 1.6162\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7295 - val_loss: 1.4077\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6571 - val_loss: 1.2436\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6096 - val_loss: 1.5098\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5862 - val_loss: 1.3734\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6163 - val_loss: 1.2350\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5659 - val_loss: 1.4143\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6036 - val_loss: 1.2097\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5191 - val_loss: 1.2272\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5018 - val_loss: 1.2878\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5392 - val_loss: 1.3243\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5524 - val_loss: 1.2187\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5048 - val_loss: 1.2192\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5011 - val_loss: 1.2110\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4854 - val_loss: 1.2100\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4961 - val_loss: 1.2195\n",
      "Epoch 191/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4860 - val_loss: 1.2147\n",
      "Epoch 192/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4877 - val_loss: 1.2219\n",
      "Epoch 193/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4961 - val_loss: 1.2050\n",
      "Epoch 194/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4944 - val_loss: 1.3079\n",
      "Epoch 195/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5082 - val_loss: 1.2215\n",
      "Epoch 196/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4645 - val_loss: 1.2210\n",
      "Epoch 197/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4838 - val_loss: 1.2547\n",
      "Epoch 198/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4675 - val_loss: 1.2076\n",
      "Epoch 199/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4708 - val_loss: 1.1839\n",
      "Epoch 200/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4545 - val_loss: 1.1985\n",
      "Epoch 201/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4888 - val_loss: 1.2366\n",
      "Epoch 202/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5154 - val_loss: 1.2123\n",
      "Epoch 203/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4889 - val_loss: 1.2172\n",
      "Epoch 204/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4580 - val_loss: 1.2213\n",
      "Epoch 205/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4542 - val_loss: 1.1806\n",
      "Epoch 206/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4342 - val_loss: 1.2472\n",
      "Epoch 207/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4722 - val_loss: 1.1908\n",
      "Epoch 208/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4443 - val_loss: 1.1833\n",
      "Epoch 209/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4789 - val_loss: 1.2178\n",
      "Epoch 210/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4691 - val_loss: 1.2241\n",
      "Epoch 211/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4537 - val_loss: 1.1837\n",
      "Epoch 212/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4566 - val_loss: 1.1683\n",
      "Epoch 213/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4374 - val_loss: 1.1761\n",
      "Epoch 214/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4669 - val_loss: 1.3122\n",
      "Epoch 215/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4743 - val_loss: 1.1833\n",
      "Epoch 216/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4924 - val_loss: 1.2588\n",
      "Epoch 217/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4612 - val_loss: 1.7494\n",
      "Epoch 218/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6610 - val_loss: 1.1853\n",
      "Epoch 219/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5194 - val_loss: 1.2701\n",
      "Epoch 220/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5386 - val_loss: 1.3145\n",
      "Epoch 221/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4975 - val_loss: 1.1848\n",
      "Epoch 222/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4758 - val_loss: 1.2460\n",
      "Epoch 223/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4311 - val_loss: 1.2654\n",
      "Epoch 224/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4567 - val_loss: 1.1921\n",
      "Epoch 225/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4273 - val_loss: 1.1523\n",
      "Epoch 226/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4329 - val_loss: 1.2459\n",
      "Epoch 227/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4188 - val_loss: 1.5142\n",
      "Epoch 228/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4809 - val_loss: 1.3331\n",
      "Epoch 229/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4943 - val_loss: 1.1615\n",
      "Epoch 230/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4492 - val_loss: 1.3476\n",
      "Epoch 231/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4669 - val_loss: 1.1737\n",
      "Epoch 232/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4340 - val_loss: 1.2056\n",
      "Epoch 233/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4533 - val_loss: 1.1666\n",
      "Epoch 234/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4308 - val_loss: 1.2991\n",
      "Epoch 235/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3878 - val_loss: 1.2599\n",
      "Epoch 236/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4804 - val_loss: 1.1836\n",
      "Epoch 237/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4530 - val_loss: 1.3876\n",
      "Epoch 238/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4734 - val_loss: 1.1689\n",
      "Epoch 239/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4983 - val_loss: 1.2858\n",
      "Epoch 240/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4578 - val_loss: 1.5927\n",
      "Epoch 241/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5868 - val_loss: 1.1782\n",
      "Epoch 242/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4567 - val_loss: 1.2472\n",
      "Epoch 243/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4566 - val_loss: 1.2330\n",
      "Epoch 244/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4345 - val_loss: 1.2163\n",
      "Epoch 245/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4163 - val_loss: 1.1491\n",
      "Epoch 246/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4050 - val_loss: 1.2072\n",
      "Epoch 247/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4792 - val_loss: 1.2081\n",
      "Epoch 248/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4083 - val_loss: 1.1423\n",
      "Epoch 249/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4113 - val_loss: 1.2434\n",
      "Epoch 250/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4597 - val_loss: 1.1505\n",
      "Epoch 251/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3629 - val_loss: 1.1334\n",
      "Epoch 252/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4087 - val_loss: 1.3036\n",
      "Epoch 253/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5073 - val_loss: 1.1494\n",
      "Epoch 254/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4284 - val_loss: 1.4331\n",
      "Epoch 255/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5129 - val_loss: 1.1818\n",
      "Epoch 256/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4330 - val_loss: 1.2658\n",
      "Epoch 257/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4561 - val_loss: 1.2833\n",
      "Epoch 258/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4632 - val_loss: 1.2358\n",
      "Epoch 259/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3850 - val_loss: 1.1357\n",
      "Epoch 260/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3670 - val_loss: 1.1280\n",
      "Epoch 261/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3548 - val_loss: 1.1924\n",
      "Epoch 262/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3876 - val_loss: 1.1516\n",
      "Epoch 263/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3770 - val_loss: 1.1253\n",
      "Epoch 264/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3595 - val_loss: 1.1874\n",
      "Epoch 265/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4178 - val_loss: 1.1346\n",
      "Epoch 266/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3725 - val_loss: 1.1241\n",
      "Epoch 267/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3427 - val_loss: 1.2095\n",
      "Epoch 268/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3742 - val_loss: 1.1189\n",
      "Epoch 269/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3519 - val_loss: 1.1550\n",
      "Epoch 270/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3894 - val_loss: 1.3119\n",
      "Epoch 271/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4085 - val_loss: 1.1291\n",
      "Epoch 272/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3556 - val_loss: 1.1747\n",
      "Epoch 273/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3849 - val_loss: 1.1355\n",
      "Epoch 274/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3640 - val_loss: 1.2319\n",
      "Epoch 275/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4398 - val_loss: 1.1578\n",
      "Epoch 276/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3518 - val_loss: 1.1137\n",
      "Epoch 277/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3418 - val_loss: 1.2084\n",
      "Epoch 278/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3738 - val_loss: 1.1373\n",
      "Epoch 279/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3282 - val_loss: 1.1249\n",
      "Epoch 280/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3549 - val_loss: 1.2168\n",
      "Epoch 281/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4145 - val_loss: 1.2312\n",
      "Epoch 282/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4786 - val_loss: 1.3512\n",
      "Epoch 283/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4843 - val_loss: 1.1445\n",
      "Epoch 284/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4454 - val_loss: 1.5752\n",
      "Epoch 285/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5347 - val_loss: 1.1677\n",
      "Epoch 286/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5050 - val_loss: 1.1890\n",
      "Epoch 287/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4026 - val_loss: 1.1684\n",
      "Epoch 288/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4036 - val_loss: 1.1131\n",
      "Epoch 289/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3293 - val_loss: 1.1087\n",
      "Epoch 290/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3168 - val_loss: 1.1133\n",
      "Epoch 291/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3378 - val_loss: 1.1632\n",
      "Epoch 292/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3510 - val_loss: 1.1775\n",
      "Epoch 293/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3468 - val_loss: 1.1870\n",
      "Epoch 294/300\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3503 - val_loss: 1.1885\n",
      "Epoch 295/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3793 - val_loss: 1.1635\n",
      "Epoch 296/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3324 - val_loss: 1.1120\n",
      "Epoch 297/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3230 - val_loss: 1.1120\n",
      "Epoch 298/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3088 - val_loss: 1.0974\n",
      "Epoch 299/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3165 - val_loss: 1.1088\n",
      "Epoch 300/300\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3213 - val_loss: 1.1682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29005d180>"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')    \n",
    "model.fit(x=X_train,y=y_train.values,\n",
    "        validation_data=(X_test,y_test.values),\n",
    "        batch_size=30,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1fbef715",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_model('../code/no_time_aggregated_model_umich_use_for_transfer_learning_jul5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "7688b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "d8bc6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Sequential()\n",
    "\n",
    "base_model.trainable = False\n",
    "    \n",
    "new_model.add(base_model)\n",
    "new_model.add(Dense(20, activation='relu'))\n",
    "new_model.add(Dense(30, activation='relu'))\n",
    "new_model.add(Dense(10, activation='relu'))\n",
    "new_model.add(Dense(1))\n",
    "              \n",
    "new_model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "1161a1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 8800.8242 - val_loss: 6275.5708\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 6273.8408 - val_loss: 4226.9902\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 4224.4482 - val_loss: 2631.5229\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2631.9209 - val_loss: 1465.2974\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1464.4387 - val_loss: 700.3030\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 710.9039 - val_loss: 360.2366\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 374.1739 - val_loss: 155.0966\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 164.2881 - val_loss: 62.2179\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 70.4987 - val_loss: 50.2333\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 53.9376 - val_loss: 85.9667\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 88.3716 - val_loss: 138.5811\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 135.5449 - val_loss: 184.7096\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 179.1556 - val_loss: 212.9824\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 205.5476 - val_loss: 218.7849\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 209.8983 - val_loss: 203.3080\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 193.9661 - val_loss: 173.1337\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 164.3528 - val_loss: 137.3884\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 129.8459 - val_loss: 103.2420\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 98.3228 - val_loss: 76.0230\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 73.8874 - val_loss: 58.3130\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 58.3250 - val_loss: 49.4501\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 52.0028 - val_loss: 47.3059\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 52.3723 - val_loss: 49.0112\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 54.5861 - val_loss: 51.7515\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 58.3622 - val_loss: 53.9930\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 60.9247 - val_loss: 54.8863\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 61.9418 - val_loss: 54.2462\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 61.0100 - val_loss: 52.3372\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 58.5709 - val_loss: 50.0977\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 55.9542 - val_loss: 48.3659\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 53.3380 - val_loss: 47.4547\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 52.0896 - val_loss: 47.3205\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.0451 - val_loss: 47.7331\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1360 - val_loss: 48.4883\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 51.5609 - val_loss: 49.0750\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.7644 - val_loss: 49.1578\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.7715 - val_loss: 48.9223\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.6425 - val_loss: 48.6300\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 51.5061 - val_loss: 48.4385\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 51.3626 - val_loss: 48.3107\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.3579 - val_loss: 48.2717\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.2890 - val_loss: 48.3214\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.3029 - val_loss: 48.2122\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2526 - val_loss: 48.0206\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.2388 - val_loss: 47.9534\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 51.1448 - val_loss: 48.0354\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1901 - val_loss: 48.0429\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1732 - val_loss: 47.9096\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.1270 - val_loss: 47.7407\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.0606 - val_loss: 47.5671\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.0776 - val_loss: 47.4343\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.0902 - val_loss: 47.3809\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1192 - val_loss: 47.3828\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1017 - val_loss: 47.4340\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.0669 - val_loss: 47.5328\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.0345 - val_loss: 47.7175\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.1508 - val_loss: 47.8651\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 51.1147 - val_loss: 47.8464\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1112 - val_loss: 47.8171\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0998 - val_loss: 47.7921\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0921 - val_loss: 47.7473\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0798 - val_loss: 47.6845\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0752 - val_loss: 47.6406\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0637 - val_loss: 47.6508\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0647 - val_loss: 47.7878\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0964 - val_loss: 48.0454\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1584 - val_loss: 48.2993\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2891 - val_loss: 48.5579\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.4145 - val_loss: 48.7526\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.5269 - val_loss: 48.9090\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.6233 - val_loss: 48.8718\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.5890 - val_loss: 48.5989\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.3898 - val_loss: 48.1245\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2365 - val_loss: 47.6830\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.0607 - val_loss: 47.4545\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.0638 - val_loss: 47.3424\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1275 - val_loss: 47.2993\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1891 - val_loss: 47.2821\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.2692 - val_loss: 47.2785\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.2860 - val_loss: 47.2845\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2397 - val_loss: 47.3031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1745 - val_loss: 47.3469\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0836 - val_loss: 47.4768\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0349 - val_loss: 47.7588\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0952 - val_loss: 48.0491\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1736 - val_loss: 48.2228\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2699 - val_loss: 48.1589\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2429 - val_loss: 47.9553\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1309 - val_loss: 47.8172\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0619 - val_loss: 47.5910\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1135 - val_loss: 47.4200\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0624 - val_loss: 47.3714\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0919 - val_loss: 47.3564\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0965 - val_loss: 47.3822\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0939 - val_loss: 47.4099\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0755 - val_loss: 47.4755\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0384 - val_loss: 47.6651\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0499 - val_loss: 47.8652\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0923 - val_loss: 48.0505\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1773 - val_loss: 48.1585\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.2126 - val_loss: 48.0941\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1810 - val_loss: 47.9324\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 51.2059 - val_loss: 47.9340\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1214 - val_loss: 48.1392\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1951 - val_loss: 48.3358\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2908 - val_loss: 48.5501\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.4024 - val_loss: 48.7044\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.4918 - val_loss: 48.7586\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.5181 - val_loss: 48.5964\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.4269 - val_loss: 48.2943\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2585 - val_loss: 47.9575\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1377 - val_loss: 47.6819\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1619 - val_loss: 47.6661\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0438 - val_loss: 47.8670\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1170 - val_loss: 47.9285\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.1087 - val_loss: 47.6697\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0176 - val_loss: 47.3629\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1155 - val_loss: 47.2704\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2314 - val_loss: 47.2623\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2790 - val_loss: 47.2617\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2815 - val_loss: 47.2686\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1810 - val_loss: 47.3616\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0379 - val_loss: 47.6291\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0059 - val_loss: 48.0489\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1326 - val_loss: 48.5435\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 51.3658 - val_loss: 49.0783\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.7313 - val_loss: 49.3940\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.8997 - val_loss: 49.0448\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.6196 - val_loss: 48.2578\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1702 - val_loss: 47.5706\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2155 - val_loss: 47.3178\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0818 - val_loss: 47.2660\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1850 - val_loss: 47.2573\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3962 - val_loss: 47.2619\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3854 - val_loss: 47.2580\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2490 - val_loss: 47.3262\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0464 - val_loss: 47.5013\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0823 - val_loss: 47.7560\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0654 - val_loss: 47.8111\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0601 - val_loss: 47.6559\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.0819 - val_loss: 47.5173\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0139 - val_loss: 47.4840\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0156 - val_loss: 47.4414\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0240 - val_loss: 47.4380\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0230 - val_loss: 47.4565\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0168 - val_loss: 47.4818\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0589 - val_loss: 47.4181\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0228 - val_loss: 47.2949\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0781 - val_loss: 47.2471\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.3208 - val_loss: 47.2608\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.4308 - val_loss: 47.2518\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.3741 - val_loss: 47.2469\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2415 - val_loss: 47.2856\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1098 - val_loss: 47.3880\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0417 - val_loss: 47.5137\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0353 - val_loss: 47.5284\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 50.9970 - val_loss: 47.4248\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0095 - val_loss: 47.3346\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0377 - val_loss: 47.2633\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1258 - val_loss: 47.2453\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.3638 - val_loss: 47.2778\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.5246 - val_loss: 47.2885\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.5692 - val_loss: 47.2942\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.5819 - val_loss: 47.2664\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.4591 - val_loss: 47.2422\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1145 - val_loss: 47.4498\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0208 - val_loss: 48.1122\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1809 - val_loss: 48.7639\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.4807 - val_loss: 49.2735\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.8224 - val_loss: 49.5508\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.9932 - val_loss: 49.3561\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.8443 - val_loss: 48.8083\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3897 - val_loss: 47.8639\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2856 - val_loss: 47.3186\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0199 - val_loss: 47.2336\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3377 - val_loss: 47.2440\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.3806 - val_loss: 47.2340\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2030 - val_loss: 47.3264\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9815 - val_loss: 47.6532\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0344 - val_loss: 48.1454\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2237 - val_loss: 48.3074\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2865 - val_loss: 48.2901\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2814 - val_loss: 48.1049\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1235 - val_loss: 47.6107\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0747 - val_loss: 47.3792\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0141 - val_loss: 47.3497\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0125 - val_loss: 47.4188\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9923 - val_loss: 47.5641\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9995 - val_loss: 47.6418\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9956 - val_loss: 47.6401\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9970 - val_loss: 47.6928\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0006 - val_loss: 47.8163\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0384 - val_loss: 47.9126\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0720 - val_loss: 47.8908\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0757 - val_loss: 47.8448\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0458 - val_loss: 47.8588\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0496 - val_loss: 47.8195\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0458 - val_loss: 47.7846\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0259 - val_loss: 47.8365\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0405 - val_loss: 47.8927\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0640 - val_loss: 47.7688\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9918 - val_loss: 47.4820\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0326 - val_loss: 47.3353\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0311 - val_loss: 47.4087\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9550 - val_loss: 47.7085\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9969 - val_loss: 48.0781\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1143 - val_loss: 48.4343\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3464 - val_loss: 48.5011\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 51.3154 - val_loss: 48.0616\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1101 - val_loss: 47.6081\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9716 - val_loss: 47.3462\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0441 - val_loss: 47.2749\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0379 - val_loss: 47.3194\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9716 - val_loss: 47.5309\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0953 - val_loss: 47.7833\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0137 - val_loss: 47.6555\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9927 - val_loss: 47.4841\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9643 - val_loss: 47.4183\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 50.9824 - val_loss: 47.4959\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9393 - val_loss: 47.7790\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9670 - val_loss: 48.2735\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1972 - val_loss: 48.9429\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.5685 - val_loss: 49.4810\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.9218 - val_loss: 49.7907\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 52.1505 - val_loss: 49.1436\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.5951 - val_loss: 47.8597\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0334 - val_loss: 47.2529\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2343 - val_loss: 47.2068\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3127 - val_loss: 47.2159\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.3720 - val_loss: 47.2226\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.3892 - val_loss: 47.2019\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1960 - val_loss: 47.3135\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 50.9831 - val_loss: 47.5645\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 50.9331 - val_loss: 47.9010\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0423 - val_loss: 48.2401\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2295 - val_loss: 48.1691\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1360 - val_loss: 47.7258\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9317 - val_loss: 47.3472\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0590 - val_loss: 47.2162\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.1245 - val_loss: 47.2602\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0020 - val_loss: 47.5190\n",
      "Epoch 242/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9213 - val_loss: 47.9252\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1199 - val_loss: 48.1260\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1454 - val_loss: 48.0154\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0911 - val_loss: 48.0041\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0833 - val_loss: 47.8635\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9650 - val_loss: 47.4324\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0568 - val_loss: 47.2215\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0501 - val_loss: 47.1929\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1467 - val_loss: 47.1948\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3420 - val_loss: 47.1916\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2962 - val_loss: 47.2097\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0245 - val_loss: 47.4052\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.8595 - val_loss: 48.1071\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0904 - val_loss: 49.0992\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.7991 - val_loss: 49.3228\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.8297 - val_loss: 48.6719\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3925 - val_loss: 48.0636\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0234 - val_loss: 47.4712\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9834 - val_loss: 47.2068\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.0163 - val_loss: 47.2027\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.4388 - val_loss: 47.3071\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.7052 - val_loss: 47.2143\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.3965 - val_loss: 47.2352\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9784 - val_loss: 47.6275\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1617 - val_loss: 47.8592\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0524 - val_loss: 47.7405\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9783 - val_loss: 47.6349\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9218 - val_loss: 47.3836\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.8577 - val_loss: 47.1848\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2094 - val_loss: 47.2255\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.4881 - val_loss: 47.2223\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.5526 - val_loss: 47.2121\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.4648 - val_loss: 47.1893\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.2988 - val_loss: 47.2214\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.8673 - val_loss: 47.7552\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0029 - val_loss: 48.8899\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.5583 - val_loss: 49.5769\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.9701 - val_loss: 49.2673\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 51.5932 - val_loss: 47.8696\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.9451 - val_loss: 47.1718\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2225 - val_loss: 47.6796\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 52.5123 - val_loss: 48.1852\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 53.2874 - val_loss: 47.7474\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 52.5014 - val_loss: 47.1757\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2645 - val_loss: 47.5128\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 50.7921 - val_loss: 48.6556\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 51.7477 - val_loss: 49.3029\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.7353 - val_loss: 48.4466\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2884 - val_loss: 47.6625\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 50.8888 - val_loss: 47.2671\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0323 - val_loss: 47.1709\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.0777 - val_loss: 47.1670\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1148 - val_loss: 47.2373\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 50.9868 - val_loss: 47.5960\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 50.8742 - val_loss: 48.0812\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1620 - val_loss: 48.3454\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.2531 - val_loss: 48.2954\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1875 - val_loss: 48.2470\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 51.1719 - val_loss: 48.0033\n"
     ]
    }
   ],
   "source": [
    "result = new_model.fit(x=X_train_scaled,y=y_train.values,\n",
    "          validation_data=(X_test_scaled,y_test.values),\n",
    "          batch_size=128,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "3b5ca613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(result.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "0101212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 380us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.00333041624862"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, new_model.predict(X_test_scaled))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
